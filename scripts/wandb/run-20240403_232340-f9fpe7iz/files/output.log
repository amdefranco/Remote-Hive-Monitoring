C:\Users\mateo\Desktop\ABBY GOES HERE\bees\scripts\mlp.py:99: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  X = np.array([data[0] for data in dataset])
C:\Users\mateo\Desktop\ABBY GOES HERE\bees\scripts\mlp.py:99: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  X = np.array([data[0] for data in dataset])
C:\Users\mateo\Desktop\ABBY GOES HERE\bees\scripts\mlp.py:107: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  train_set = [(torch.tensor(X_train[i]), torch.tensor(y_train[i])) for i in range(len(X_train))]
C:\Users\mateo\Desktop\ABBY GOES HERE\bees\scripts\mlp.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_set = [(torch.tensor(X_val[i]), torch.tensor(y_val[i])) for i in range(len(X_val))]
hive temp
hive humidity
hive pressure
weather temp
weather humidity
weather pressure
wind speed
cloud coverage
rain
Epoch 1/200, Loss: 3.027853012084961
Epoch 2/200, Loss: 2.960763931274414
Epoch 3/200, Loss: 2.9035388231277466
Epoch 4/200, Loss: 2.8394885063171387
Epoch 5/200, Loss: 2.7794147729873657
Epoch 6/200, Loss: 2.719671130180359
Epoch 7/200, Loss: 2.667240619659424
Epoch 8/200, Loss: 2.6093209981918335
Epoch 9/200, Loss: 2.5640366077423096
Epoch 10/200, Loss: 2.515293836593628
Epoch 11/200, Loss: 2.474248766899109
Epoch 12/200, Loss: 2.425026535987854
Epoch 13/200, Loss: 2.38338565826416
Epoch 14/200, Loss: 2.3364354372024536
Epoch 15/200, Loss: 2.310112237930298
Epoch 16/200, Loss: 2.2656469345092773
Epoch 17/200, Loss: 2.2364193201065063
Epoch 18/200, Loss: 2.2149112224578857
Epoch 19/200, Loss: 2.186258554458618
Epoch 20/200, Loss: 2.1520766019821167
Epoch 21/200, Loss: 2.131353735923767
Epoch 22/200, Loss: 2.1120107173919678
Epoch 23/200, Loss: 2.0845030546188354
Epoch 24/200, Loss: 2.0434680581092834
Epoch 25/200, Loss: 2.039314568042755
Epoch 26/200, Loss: 2.0316197872161865
Epoch 27/200, Loss: 2.026810944080353
Epoch 28/200, Loss: 1.9921113848686218
Epoch 29/200, Loss: 1.9818962216377258
Epoch 30/200, Loss: 1.978303611278534
Epoch 31/200, Loss: 1.9489370584487915
Epoch 32/200, Loss: 1.953709602355957
Epoch 33/200, Loss: 1.9532590508460999
Epoch 34/200, Loss: 1.9195839762687683
Epoch 35/200, Loss: 1.9163283705711365
Epoch 36/200, Loss: 1.8967920541763306
Epoch 37/200, Loss: 1.875563144683838
Epoch 38/200, Loss: 1.8675241470336914
Epoch 39/200, Loss: 1.8545282483100891
Epoch 40/200, Loss: 1.8735451102256775
Epoch 41/200, Loss: 1.8532288670539856
Epoch 42/200, Loss: 1.8386873602867126
Epoch 43/200, Loss: 1.8287648558616638
Epoch 44/200, Loss: 1.8287887573242188
Epoch 45/200, Loss: 1.8012762665748596
Epoch 46/200, Loss: 1.7843958735466003
Epoch 47/200, Loss: 1.7831575870513916
Epoch 48/200, Loss: 1.7542389631271362
Epoch 49/200, Loss: 1.764046311378479
Epoch 50/200, Loss: 1.774061143398285
Epoch 51/200, Loss: 1.7411010265350342
Epoch 52/200, Loss: 1.7501574754714966
Epoch 53/200, Loss: 1.7290634512901306
Epoch 54/200, Loss: 1.7268975973129272
Epoch 55/200, Loss: 1.710738718509674
Epoch 56/200, Loss: 1.7251333594322205
Epoch 57/200, Loss: 1.7293792963027954
Epoch 58/200, Loss: 1.7163132429122925
Epoch 59/200, Loss: 1.6841226816177368
Epoch 60/200, Loss: 1.6661407947540283
Epoch 61/200, Loss: 1.6596518158912659
Epoch 62/200, Loss: 1.6793777346611023
Epoch 63/200, Loss: 1.6670241355895996
Epoch 64/200, Loss: 1.6777510046958923
Epoch 65/200, Loss: 1.6537793278694153
Epoch 66/200, Loss: 1.6616808772087097
Epoch 67/200, Loss: 1.637669026851654
Epoch 68/200, Loss: 1.625481128692627
Epoch 69/200, Loss: 1.633356750011444
Epoch 70/200, Loss: 1.628972351551056
Epoch 71/200, Loss: 1.5982069969177246
Epoch 72/200, Loss: 1.609212577342987
Epoch 73/200, Loss: 1.5963939428329468
Epoch 74/200, Loss: 1.5751109719276428
Epoch 75/200, Loss: 1.598778486251831
Epoch 76/200, Loss: 1.574460506439209
Epoch 77/200, Loss: 1.5705789923667908
Epoch 78/200, Loss: 1.5695040822029114
Epoch 79/200, Loss: 1.573792815208435
Epoch 80/200, Loss: 1.552869200706482
Epoch 81/200, Loss: 1.53859543800354
Epoch 82/200, Loss: 1.5521526336669922
Epoch 83/200, Loss: 1.5247292518615723
Epoch 84/200, Loss: 1.5324244499206543
Epoch 85/200, Loss: 1.5129677653312683
Epoch 86/200, Loss: 1.5062555074691772
Epoch 87/200, Loss: 1.5066572427749634
Epoch 88/200, Loss: 1.4927575588226318
Epoch 89/200, Loss: 1.5160571932792664
Epoch 90/200, Loss: 1.4922780990600586
Epoch 91/200, Loss: 1.4858106970787048
Epoch 92/200, Loss: 1.4813750982284546
Epoch 93/200, Loss: 1.4985833764076233
Epoch 94/200, Loss: 1.4587162733078003
Epoch 95/200, Loss: 1.4616860151290894
Epoch 96/200, Loss: 1.4508740901947021
Epoch 97/200, Loss: 1.4762411713600159
Epoch 98/200, Loss: 1.4429123401641846
Epoch 99/200, Loss: 1.453088402748108
Epoch 100/200, Loss: 1.4672478437423706
Epoch 101/200, Loss: 1.416503369808197
Epoch 102/200, Loss: 1.4203569293022156
Epoch 103/200, Loss: 1.4293175339698792
Epoch 104/200, Loss: 1.416074812412262
Epoch 105/200, Loss: 1.4075470566749573
Epoch 106/200, Loss: 1.4235328435897827
Epoch 107/200, Loss: 1.4114487171173096
Epoch 108/200, Loss: 1.3906104564666748
Epoch 109/200, Loss: 1.3846522569656372
Epoch 110/200, Loss: 1.3705686330795288
Epoch 111/200, Loss: 1.3870160579681396
Epoch 112/200, Loss: 1.3825662732124329
Epoch 113/200, Loss: 1.3869903087615967
Epoch 114/200, Loss: 1.382899284362793
Epoch 115/200, Loss: 1.3565253615379333
Epoch 116/200, Loss: 1.331899642944336
Epoch 117/200, Loss: 1.381459653377533
Epoch 118/200, Loss: 1.3632636666297913
Epoch 119/200, Loss: 1.343692660331726
Epoch 120/200, Loss: 1.3498536348342896
Epoch 121/200, Loss: 1.3398405313491821
Epoch 122/200, Loss: 1.3160011768341064
Epoch 123/200, Loss: 1.3231796026229858
Epoch 124/200, Loss: 1.307650089263916
Epoch 125/200, Loss: 1.3377988934516907
Epoch 126/200, Loss: 1.3149864077568054
Epoch 127/200, Loss: 1.2982669472694397
Epoch 128/200, Loss: 1.3031616806983948
Epoch 129/200, Loss: 1.2901150584220886
Epoch 130/200, Loss: 1.305864930152893
Epoch 131/200, Loss: 1.2992073893547058
Epoch 132/200, Loss: 1.2924333810806274
Epoch 133/200, Loss: 1.2778497338294983
Epoch 134/200, Loss: 1.2605690956115723
Epoch 135/200, Loss: 1.2689974308013916
Epoch 136/200, Loss: 1.2875663042068481
Epoch 137/200, Loss: 1.2606338262557983
Epoch 138/200, Loss: 1.2662792205810547
Epoch 139/200, Loss: 1.253102421760559
Epoch 140/200, Loss: 1.243038833141327
Epoch 141/200, Loss: 1.249489665031433
Epoch 142/200, Loss: 1.2599908113479614
Epoch 143/200, Loss: 1.2413637042045593
Epoch 144/200, Loss: 1.2424534559249878
Epoch 145/200, Loss: 1.2078620791435242
Epoch 146/200, Loss: 1.2362301349639893
Epoch 147/200, Loss: 1.2247832417488098
Epoch 148/200, Loss: 1.2133830189704895
Epoch 149/200, Loss: 1.2050880789756775
Epoch 150/200, Loss: 1.2340924143791199
Epoch 151/200, Loss: 1.2258157134056091
Epoch 152/200, Loss: 1.2109332084655762
Epoch 153/200, Loss: 1.208371102809906
Epoch 154/200, Loss: 1.213157594203949
Epoch 155/200, Loss: 1.201214611530304
Epoch 156/200, Loss: 1.186737835407257
Epoch 157/200, Loss: 1.198241114616394
Epoch 158/200, Loss: 1.207107424736023
Epoch 159/200, Loss: 1.1842289566993713
Epoch 160/200, Loss: 1.172954797744751
Epoch 161/200, Loss: 1.169929027557373
Epoch 162/200, Loss: 1.186545491218567
Epoch 163/200, Loss: 1.1727824807167053
Epoch 164/200, Loss: 1.167996108531952
Epoch 165/200, Loss: 1.155775010585785
Epoch 166/200, Loss: 1.1289304196834564
Epoch 167/200, Loss: 1.158674418926239
Epoch 168/200, Loss: 1.1742450594902039
Epoch 169/200, Loss: 1.1523478031158447
Epoch 170/200, Loss: 1.1401572823524475
Epoch 171/200, Loss: 1.1373647451400757
Epoch 172/200, Loss: 1.1284949779510498
Epoch 173/200, Loss: 1.1441587209701538
Epoch 174/200, Loss: 1.125287652015686
Epoch 175/200, Loss: 1.1371748447418213
Epoch 176/200, Loss: 1.1238173246383667
Epoch 177/200, Loss: 1.1193671226501465
Epoch 178/200, Loss: 1.1203966736793518
Epoch 179/200, Loss: 1.1269350051879883
Epoch 180/200, Loss: 1.102575659751892
Epoch 181/200, Loss: 1.0886286795139313
Epoch 182/200, Loss: 1.0876272022724152
Epoch 183/200, Loss: 1.1123379468917847
Epoch 184/200, Loss: 1.081510841846466
Epoch 185/200, Loss: 1.0730994045734406
Epoch 186/200, Loss: 1.0981794595718384
Epoch 187/200, Loss: 1.0647562444210052
Epoch 188/200, Loss: 1.1012494564056396
Epoch 189/200, Loss: 1.087937831878662
Epoch 190/200, Loss: 1.0941531658172607
Epoch 191/200, Loss: 1.0664294362068176
Epoch 192/200, Loss: 1.0922616124153137
Epoch 193/200, Loss: 1.065877377986908
Epoch 194/200, Loss: 1.0658366084098816
Epoch 195/200, Loss: 1.0461674332618713
Epoch 196/200, Loss: 1.074665129184723
Epoch 197/200, Loss: 1.0650962591171265
Epoch 198/200, Loss: 1.0357325673103333
Epoch 199/200, Loss: 1.0461533665657043
Epoch 200/200, Loss: 1.06080362200737
Validation Accuracy: 0.8245614035087719