C:\Users\mateo\Desktop\ABBY GOES HERE\bees\scripts\mlp.py:100: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  X = np.array([data[0] for data in dataset])
C:\Users\mateo\Desktop\ABBY GOES HERE\bees\scripts\mlp.py:100: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  X = np.array([data[0] for data in dataset])
C:\Users\mateo\Desktop\ABBY GOES HERE\bees\scripts\mlp.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  train_set = [(torch.tensor(X_train[i]), torch.tensor(y_train[i])) for i in range(len(X_train))]
C:\Users\mateo\Desktop\ABBY GOES HERE\bees\scripts\mlp.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_set = [(torch.tensor(X_val[i]), torch.tensor(y_val[i])) for i in range(len(X_val))]
hive temp
hive humidity
hive pressure
weather temp
weather humidity
weather pressure
wind speed
cloud coverage
rain
Epoch 1/200, Loss: 14.857264041900635
Epoch 2/200, Loss: 13.474844336509705
Epoch 3/200, Loss: 12.356894373893738
Epoch 4/200, Loss: 11.613981127738953
Epoch 5/200, Loss: 10.740331947803497
Epoch 6/200, Loss: 10.403846800327301
Epoch 7/200, Loss: 10.088429510593414
Epoch 8/200, Loss: 10.041816055774689
Epoch 9/200, Loss: 9.334823608398438
Epoch 10/200, Loss: 9.444964289665222
Epoch 11/200, Loss: 9.343255996704102
Epoch 12/200, Loss: 9.395312070846558
Epoch 13/200, Loss: 8.982407033443451
Epoch 14/200, Loss: 8.657383501529694
Epoch 15/200, Loss: 8.973201274871826
Epoch 16/200, Loss: 8.648062229156494
Epoch 17/200, Loss: 8.477368354797363
Epoch 18/200, Loss: 8.548930823802948
Epoch 19/200, Loss: 8.292483627796173
Epoch 20/200, Loss: 8.60255080461502
Epoch 21/200, Loss: 8.079832673072815
Epoch 22/200, Loss: 8.065307259559631
Epoch 23/200, Loss: 8.278640270233154
Epoch 24/200, Loss: 7.772819757461548
Epoch 25/200, Loss: 7.710949897766113
Epoch 26/200, Loss: 7.762792706489563
Epoch 27/200, Loss: 7.608567178249359
Epoch 28/200, Loss: 7.569401562213898
Epoch 29/200, Loss: 7.303858995437622
Epoch 30/200, Loss: 7.462508141994476
Epoch 31/200, Loss: 7.756258308887482
Epoch 32/200, Loss: 7.277707457542419
Epoch 33/200, Loss: 7.05220028758049
Epoch 34/200, Loss: 7.13918536901474
Epoch 35/200, Loss: 7.0013973116874695
Epoch 36/200, Loss: 7.003781735897064
Epoch 37/200, Loss: 6.873962819576263
Epoch 38/200, Loss: 7.208563387393951
Epoch 39/200, Loss: 6.854994088411331
Epoch 40/200, Loss: 6.9836441576480865
Epoch 41/200, Loss: 6.619793057441711
Epoch 42/200, Loss: 6.7564085721969604
Epoch 43/200, Loss: 6.651794999837875
Epoch 44/200, Loss: 6.7403329610824585
Epoch 45/200, Loss: 6.962051868438721
Epoch 46/200, Loss: 6.488845378160477
Epoch 47/200, Loss: 6.547329902648926
Epoch 48/200, Loss: 6.3431790471076965
Epoch 49/200, Loss: 6.47886523604393
Epoch 50/200, Loss: 6.365286827087402
Epoch 51/200, Loss: 6.244866371154785
Epoch 52/200, Loss: 6.217933475971222
Epoch 53/200, Loss: 6.2784039080142975
Epoch 54/200, Loss: 6.097459822893143
Epoch 55/200, Loss: 6.069791615009308
Epoch 56/200, Loss: 6.274746388196945
Epoch 57/200, Loss: 6.4361851811409
Epoch 58/200, Loss: 6.236713916063309
Epoch 59/200, Loss: 5.8169068694114685
Epoch 60/200, Loss: 5.965284526348114
Epoch 61/200, Loss: 5.727525621652603
Epoch 62/200, Loss: 5.909213185310364
Epoch 63/200, Loss: 6.065622687339783
Epoch 64/200, Loss: 5.7783390283584595
Epoch 65/200, Loss: 5.6483482122421265
Epoch 66/200, Loss: 5.601757317781448
Epoch 67/200, Loss: 5.759498417377472
Epoch 68/200, Loss: 5.40262570977211
Epoch 69/200, Loss: 5.720599949359894
Epoch 70/200, Loss: 5.310382932424545
Epoch 71/200, Loss: 5.5155452489852905
Epoch 72/200, Loss: 5.626141399145126
Epoch 73/200, Loss: 5.644531667232513
Epoch 74/200, Loss: 5.393745958805084
Epoch 75/200, Loss: 5.447924613952637
Epoch 76/200, Loss: 5.238470524549484
Epoch 77/200, Loss: 5.3743418753147125
Epoch 78/200, Loss: 5.050375834107399
Epoch 79/200, Loss: 5.142304480075836
Epoch 80/200, Loss: 5.189464271068573
Epoch 81/200, Loss: 5.295008182525635
Epoch 82/200, Loss: 5.227220267057419
Epoch 83/200, Loss: 5.081855028867722
Epoch 84/200, Loss: 5.211810201406479
Epoch 85/200, Loss: 5.322837561368942
Epoch 86/200, Loss: 5.04402431845665
Epoch 87/200, Loss: 4.86503866314888
Epoch 88/200, Loss: 5.03631404042244
Epoch 89/200, Loss: 4.990064859390259
Epoch 90/200, Loss: 5.142762690782547
Epoch 91/200, Loss: 4.995965749025345
Epoch 92/200, Loss: 4.8633080422878265
Epoch 93/200, Loss: 5.024806886911392
Epoch 94/200, Loss: 4.94766229391098
Epoch 95/200, Loss: 4.794743061065674
Epoch 96/200, Loss: 5.038848012685776
Epoch 97/200, Loss: 4.95415660738945
Epoch 98/200, Loss: 4.723308205604553
Epoch 99/200, Loss: 4.728763103485107
Epoch 100/200, Loss: 4.651250064373016
Epoch 101/200, Loss: 4.590734273195267
Epoch 102/200, Loss: 4.580556958913803
Epoch 103/200, Loss: 4.712856769561768
Epoch 104/200, Loss: 4.563796699047089
Epoch 105/200, Loss: 4.473701700568199
Epoch 106/200, Loss: 4.426728576421738
Epoch 107/200, Loss: 4.546597272157669
Epoch 108/200, Loss: 4.487042993307114
Epoch 109/200, Loss: 4.38664311170578
Epoch 110/200, Loss: 4.5072380900383
Epoch 111/200, Loss: 4.452130228281021
Epoch 112/200, Loss: 4.259769551455975
Epoch 113/200, Loss: 4.3946225345134735
Epoch 114/200, Loss: 4.462926536798477
Epoch 115/200, Loss: 4.530636817216873
Epoch 116/200, Loss: 4.411852985620499
Epoch 117/200, Loss: 4.335157558321953
Epoch 118/200, Loss: 4.3764336705207825
Epoch 119/200, Loss: 4.509289920330048
Epoch 120/200, Loss: 4.514979243278503
Epoch 121/200, Loss: 4.093902334570885
Epoch 122/200, Loss: 4.589913070201874
Epoch 123/200, Loss: 4.222886085510254
Epoch 124/200, Loss: 4.033815264701843
Epoch 125/200, Loss: 4.08591902256012
Epoch 126/200, Loss: 4.120281934738159
Epoch 127/200, Loss: 4.036204069852829
Epoch 128/200, Loss: 4.022438123822212
Epoch 129/200, Loss: 4.3698737770318985
Epoch 130/200, Loss: 4.327790170907974
Epoch 131/200, Loss: 4.1324076652526855
Epoch 132/200, Loss: 4.051490142941475
Epoch 133/200, Loss: 3.9825159907341003
Epoch 134/200, Loss: 3.9969805479049683
Epoch 135/200, Loss: 3.971259266138077
Epoch 136/200, Loss: 3.995711252093315
Epoch 137/200, Loss: 4.045240521430969
Epoch 138/200, Loss: 4.13346354663372
Epoch 139/200, Loss: 3.953516334295273
Epoch 140/200, Loss: 4.091541096568108
Epoch 141/200, Loss: 3.852478861808777
Epoch 142/200, Loss: 4.052850320935249
Epoch 143/200, Loss: 3.869462937116623
Epoch 144/200, Loss: 4.05765363574028
Epoch 145/200, Loss: 3.9034945517778397
Epoch 146/200, Loss: 3.8933311104774475
Epoch 147/200, Loss: 3.8966606855392456
Epoch 148/200, Loss: 3.747033402323723
Epoch 149/200, Loss: 3.7239612340927124
Epoch 150/200, Loss: 3.8470624536275864
Epoch 151/200, Loss: 3.798873394727707
Epoch 152/200, Loss: 3.7981364876031876
Epoch 153/200, Loss: 3.7491214871406555
Epoch 154/200, Loss: 3.593211382627487
Epoch 155/200, Loss: 3.813587337732315
Epoch 156/200, Loss: 3.5905862599611282
Epoch 157/200, Loss: 3.53756220638752
Epoch 158/200, Loss: 3.6933608949184418
Epoch 159/200, Loss: 3.6636415496468544
Epoch 160/200, Loss: 3.5673506557941437
Epoch 161/200, Loss: 3.7239654511213303
Epoch 162/200, Loss: 3.494542896747589
Epoch 163/200, Loss: 3.6061874479055405
Epoch 164/200, Loss: 3.562239944934845
Epoch 165/200, Loss: 3.5004281401634216
Epoch 166/200, Loss: 3.432363733649254
Epoch 167/200, Loss: 3.5745947062969208
Epoch 168/200, Loss: 3.561628445982933
Epoch 169/200, Loss: 3.2957889325916767
Epoch 170/200, Loss: 3.7489330172538757
Epoch 171/200, Loss: 3.4501942098140717
Epoch 172/200, Loss: 3.3399911671876907
Epoch 173/200, Loss: 3.456043526530266
Epoch 174/200, Loss: 3.532701715826988
Epoch 175/200, Loss: 3.548317939043045
Epoch 176/200, Loss: 3.5729778558015823
Epoch 177/200, Loss: 3.5685941129922867
Epoch 178/200, Loss: 3.282487228512764
Epoch 179/200, Loss: 3.3021833300590515
Epoch 180/200, Loss: 3.3227925896644592
Epoch 181/200, Loss: 3.256543904542923
Epoch 182/200, Loss: 3.286834493279457
Epoch 183/200, Loss: 3.3679194301366806
Epoch 184/200, Loss: 3.217865914106369
Epoch 185/200, Loss: 3.4752852767705917
Epoch 186/200, Loss: 3.222672536969185
Epoch 187/200, Loss: 3.3425768315792084
Epoch 188/200, Loss: 3.267987310886383
Epoch 189/200, Loss: 3.2526580542325974
Epoch 190/200, Loss: 3.1580055207014084
Epoch 191/200, Loss: 3.362651541829109
Epoch 192/200, Loss: 3.3452231734991074
Epoch 193/200, Loss: 3.0775413513183594
Epoch 194/200, Loss: 3.2275299578905106
Epoch 195/200, Loss: 3.1528663337230682
Epoch 196/200, Loss: 3.086468383669853
Epoch 197/200, Loss: 3.0115838795900345
Epoch 198/200, Loss: 3.1790778934955597
Epoch 199/200, Loss: 3.33820016682148
Epoch 200/200, Loss: 3.182352766394615
Validation Accuracy: 0.7368421052631579