C:\Users\mateo\Desktop\ABBY GOES HERE\bees\scripts\mlp.py:99: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  X = np.array([data[0] for data in dataset])
C:\Users\mateo\Desktop\ABBY GOES HERE\bees\scripts\mlp.py:99: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  X = np.array([data[0] for data in dataset])
C:\Users\mateo\Desktop\ABBY GOES HERE\bees\scripts\mlp.py:107: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  train_set = [(torch.tensor(X_train[i]), torch.tensor(y_train[i])) for i in range(len(X_train))]
C:\Users\mateo\Desktop\ABBY GOES HERE\bees\scripts\mlp.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_set = [(torch.tensor(X_val[i]), torch.tensor(y_val[i])) for i in range(len(X_val))]
hive temp
hive humidity
hive pressure
weather temp
weather humidity
weather pressure
wind speed
cloud coverage
rain
Epoch 1/200, Loss: 1.316634178161621
Epoch 2/200, Loss: 1.3032872676849365
Epoch 3/200, Loss: 1.2902339696884155
Epoch 4/200, Loss: 1.2774847745895386
Epoch 5/200, Loss: 1.265043020248413
Epoch 6/200, Loss: 1.2529089450836182
Epoch 7/200, Loss: 1.2410930395126343
Epoch 8/200, Loss: 1.2295958995819092
Epoch 9/200, Loss: 1.2184087038040161
Epoch 10/200, Loss: 1.2075260877609253
Epoch 11/200, Loss: 1.196943998336792
Epoch 12/200, Loss: 1.1866596937179565
Epoch 13/200, Loss: 1.1766687631607056
Epoch 14/200, Loss: 1.1669756174087524
Epoch 15/200, Loss: 1.1575613021850586
Epoch 16/200, Loss: 1.1484251022338867
Epoch 17/200, Loss: 1.1395641565322876
Epoch 18/200, Loss: 1.1309778690338135
Epoch 19/200, Loss: 1.122665524482727
Epoch 20/200, Loss: 1.1146202087402344
Epoch 21/200, Loss: 1.1068329811096191
Epoch 22/200, Loss: 1.0992978811264038
Epoch 23/200, Loss: 1.091997742652893
Epoch 24/200, Loss: 1.0849266052246094
Epoch 25/200, Loss: 1.0780742168426514
Epoch 26/200, Loss: 1.0714410543441772
Epoch 27/200, Loss: 1.065010905265808
Epoch 28/200, Loss: 1.0587815046310425
Epoch 29/200, Loss: 1.0527427196502686
Epoch 30/200, Loss: 1.0468926429748535
Epoch 31/200, Loss: 1.0412172079086304
Epoch 32/200, Loss: 1.0357091426849365
Epoch 33/200, Loss: 1.030368447303772
Epoch 34/200, Loss: 1.0251871347427368
Epoch 35/200, Loss: 1.0201505422592163
Epoch 36/200, Loss: 1.0152488946914673
Epoch 37/200, Loss: 1.0104697942733765
Epoch 38/200, Loss: 1.0058040618896484
Epoch 39/200, Loss: 1.0012508630752563
Epoch 40/200, Loss: 0.9968047142028809
Epoch 41/200, Loss: 0.9924536347389221
Epoch 42/200, Loss: 0.9881848692893982
Epoch 43/200, Loss: 0.9839919209480286
Epoch 44/200, Loss: 0.9798856973648071
Epoch 45/200, Loss: 0.975859522819519
Epoch 46/200, Loss: 0.9718947410583496
Epoch 47/200, Loss: 0.9679927229881287
Epoch 48/200, Loss: 0.9641574621200562
Epoch 49/200, Loss: 0.9603811502456665
Epoch 50/200, Loss: 0.9566581845283508
Epoch 51/200, Loss: 0.9529924392700195
Epoch 52/200, Loss: 0.949379026889801
Epoch 53/200, Loss: 0.9458090662956238
Epoch 54/200, Loss: 0.9422866702079773
Epoch 55/200, Loss: 0.938806414604187
Epoch 56/200, Loss: 0.9353631734848022
Epoch 57/200, Loss: 0.9319557547569275
Epoch 58/200, Loss: 0.9285820126533508
Epoch 59/200, Loss: 0.9252325296401978
Epoch 60/200, Loss: 0.9219078421592712
Epoch 61/200, Loss: 0.9186152815818787
Epoch 62/200, Loss: 0.9153552055358887
Epoch 63/200, Loss: 0.9121251702308655
Epoch 64/200, Loss: 0.9089178442955017
Epoch 65/200, Loss: 0.9057348370552063
Epoch 66/200, Loss: 0.9025930166244507
Epoch 67/200, Loss: 0.8994830846786499
Epoch 68/200, Loss: 0.8963965773582458
Epoch 69/200, Loss: 0.8933300971984863
Epoch 70/200, Loss: 0.8902835249900818
Epoch 71/200, Loss: 0.8872637152671814
Epoch 72/200, Loss: 0.8842653632164001
Epoch 73/200, Loss: 0.8812991380691528
Epoch 74/200, Loss: 0.8783544301986694
Epoch 75/200, Loss: 0.875433087348938
Epoch 76/200, Loss: 0.8725382089614868
Epoch 77/200, Loss: 0.8696734309196472
Epoch 78/200, Loss: 0.8668271899223328
Epoch 79/200, Loss: 0.8640021681785583
Epoch 80/200, Loss: 0.8612000346183777
Epoch 81/200, Loss: 0.8584395051002502
Epoch 82/200, Loss: 0.8557103872299194
Epoch 83/200, Loss: 0.8530064821243286
Epoch 84/200, Loss: 0.850317656993866
Epoch 85/200, Loss: 0.8476421236991882
Epoch 86/200, Loss: 0.844977855682373
Epoch 87/200, Loss: 0.8423359990119934
Epoch 88/200, Loss: 0.8397093415260315
Epoch 89/200, Loss: 0.8370980620384216
Epoch 90/200, Loss: 0.834520161151886
Epoch 91/200, Loss: 0.831970751285553
Epoch 92/200, Loss: 0.8294367790222168
Epoch 93/200, Loss: 0.8269239664077759
Epoch 94/200, Loss: 0.8244332671165466
Epoch 95/200, Loss: 0.8219519853591919
Epoch 96/200, Loss: 0.8194802403450012
Epoch 97/200, Loss: 0.8170228004455566
Epoch 98/200, Loss: 0.8145759701728821
Epoch 99/200, Loss: 0.8121417760848999
Epoch 100/200, Loss: 0.8097220659255981
Epoch 101/200, Loss: 0.8073128461837769
Epoch 102/200, Loss: 0.8049151301383972
Epoch 103/200, Loss: 0.8025287985801697
Epoch 104/200, Loss: 0.8001527786254883
Epoch 105/200, Loss: 0.7977820038795471
Epoch 106/200, Loss: 0.7954268455505371
Epoch 107/200, Loss: 0.7930927276611328
Epoch 108/200, Loss: 0.7907710075378418
Epoch 109/200, Loss: 0.788465142250061
Epoch 110/200, Loss: 0.7861742377281189
Epoch 111/200, Loss: 0.7838947176933289
Epoch 112/200, Loss: 0.7816328406333923
Epoch 113/200, Loss: 0.7793830037117004
Epoch 114/200, Loss: 0.7771409153938293
Epoch 115/200, Loss: 0.7749074101448059
Epoch 116/200, Loss: 0.7726787328720093
Epoch 117/200, Loss: 0.7704585194587708
Epoch 118/200, Loss: 0.7682415843009949
Epoch 119/200, Loss: 0.766039252281189
Epoch 120/200, Loss: 0.7638438940048218
Epoch 121/200, Loss: 0.7616513967514038
Epoch 122/200, Loss: 0.7594708800315857
Epoch 123/200, Loss: 0.7573014497756958
Epoch 124/200, Loss: 0.7551493048667908
Epoch 125/200, Loss: 0.7530069947242737
Epoch 126/200, Loss: 0.7508726119995117
Epoch 127/200, Loss: 0.748744547367096
Epoch 128/200, Loss: 0.7466234564781189
Epoch 129/200, Loss: 0.7445167899131775
Epoch 130/200, Loss: 0.7424172163009644
Epoch 131/200, Loss: 0.7403282523155212
Epoch 132/200, Loss: 0.7382475137710571
Epoch 133/200, Loss: 0.7361730933189392
Epoch 134/200, Loss: 0.7341089844703674
Epoch 135/200, Loss: 0.7320507764816284
Epoch 136/200, Loss: 0.730003297328949
Epoch 137/200, Loss: 0.7279691696166992
Epoch 138/200, Loss: 0.7259488701820374
Epoch 139/200, Loss: 0.7239410281181335
Epoch 140/200, Loss: 0.7219449877738953
Epoch 141/200, Loss: 0.7199632525444031
Epoch 142/200, Loss: 0.7179927229881287
Epoch 143/200, Loss: 0.7160323262214661
Epoch 144/200, Loss: 0.7140793800354004
Epoch 145/200, Loss: 0.7121451497077942
Epoch 146/200, Loss: 0.7102263569831848
Epoch 147/200, Loss: 0.7083181738853455
Epoch 148/200, Loss: 0.7064205408096313
Epoch 149/200, Loss: 0.7045320868492126
Epoch 150/200, Loss: 0.7026515603065491
Epoch 151/200, Loss: 0.7007843852043152
Epoch 152/200, Loss: 0.6989212036132812
Epoch 153/200, Loss: 0.6970680952072144
Epoch 154/200, Loss: 0.6952252984046936
Epoch 155/200, Loss: 0.693394660949707
Epoch 156/200, Loss: 0.691575825214386
Epoch 157/200, Loss: 0.689764142036438
Epoch 158/200, Loss: 0.6879615783691406
Epoch 159/200, Loss: 0.6861684918403625
Epoch 160/200, Loss: 0.6843867301940918
Epoch 161/200, Loss: 0.6826114654541016
Epoch 162/200, Loss: 0.6808420419692993
Epoch 163/200, Loss: 0.6790803670883179
Epoch 164/200, Loss: 0.6773324608802795
Epoch 165/200, Loss: 0.6755919456481934
Epoch 166/200, Loss: 0.6738572120666504
Epoch 167/200, Loss: 0.6721261739730835
Epoch 168/200, Loss: 0.6704055070877075
Epoch 169/200, Loss: 0.6686910390853882
Epoch 170/200, Loss: 0.6669868230819702
Epoch 171/200, Loss: 0.6652876138687134
Epoch 172/200, Loss: 0.6635942459106445
Epoch 173/200, Loss: 0.6619098782539368
Epoch 174/200, Loss: 0.6602301597595215
Epoch 175/200, Loss: 0.65855473279953
Epoch 176/200, Loss: 0.6568842530250549
Epoch 177/200, Loss: 0.6552206873893738
Epoch 178/200, Loss: 0.6535592079162598
Epoch 179/200, Loss: 0.6519009470939636
Epoch 180/200, Loss: 0.6502476334571838
Epoch 181/200, Loss: 0.6485945582389832
Epoch 182/200, Loss: 0.6469518542289734
Epoch 183/200, Loss: 0.6453119516372681
Epoch 184/200, Loss: 0.6436713933944702
Epoch 185/200, Loss: 0.6420325636863708
Epoch 186/200, Loss: 0.640399158000946
Epoch 187/200, Loss: 0.6387696862220764
Epoch 188/200, Loss: 0.6371473073959351
Epoch 189/200, Loss: 0.6355338096618652
Epoch 190/200, Loss: 0.6339297890663147
Epoch 191/200, Loss: 0.6323372721672058
Epoch 192/200, Loss: 0.6307485699653625
Epoch 193/200, Loss: 0.6291670799255371
Epoch 194/200, Loss: 0.6275877952575684
Epoch 195/200, Loss: 0.6260179281234741
Epoch 196/200, Loss: 0.6244568228721619
Epoch 197/200, Loss: 0.6229063272476196
Epoch 198/200, Loss: 0.6213635206222534
Epoch 199/200, Loss: 0.6198232769966125
Epoch 200/200, Loss: 0.6182847023010254
Validation Accuracy: 0.7543859649122807