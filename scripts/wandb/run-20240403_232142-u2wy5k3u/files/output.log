hive temp
hive humidity
hive pressure
weather temp
weather humidity
weather pressure
wind speed
cloud coverage
rain
Epoch 1/200, Loss: 14.375768065452576
Epoch 2/200, Loss: 13.133381366729736
Epoch 3/200, Loss: 12.032711386680603
Epoch 4/200, Loss: 11.312215209007263
Epoch 5/200, Loss: 10.627828657627106
Epoch 6/200, Loss: 10.376969039440155
Epoch 7/200, Loss: 10.327729940414429
Epoch 8/200, Loss: 9.544775307178497
Epoch 9/200, Loss: 9.446449398994446
Epoch 10/200, Loss: 9.276199460029602
Epoch 11/200, Loss: 9.47390902042389
Epoch 12/200, Loss: 8.935934364795685
Epoch 13/200, Loss: 9.049948871135712
Epoch 14/200, Loss: 8.872435092926025
Epoch 15/200, Loss: 9.011255621910095
Epoch 16/200, Loss: 8.536294639110565
Epoch 17/200, Loss: 8.762542068958282
Epoch 18/200, Loss: 8.213438242673874
Epoch 19/200, Loss: 8.171485126018524
Epoch 20/200, Loss: 8.43369835615158
Epoch 21/200, Loss: 7.924225032329559
Epoch 22/200, Loss: 8.057146906852722
Epoch 23/200, Loss: 7.861144542694092
Epoch 24/200, Loss: 7.929990589618683
Epoch 25/200, Loss: 7.868261218070984
Epoch 26/200, Loss: 7.460960865020752
Epoch 27/200, Loss: 7.606531083583832
Epoch 28/200, Loss: 7.455706179141998
Epoch 29/200, Loss: 7.582908451557159
Epoch 30/200, Loss: 7.45231306552887
Epoch 31/200, Loss: 7.477453231811523
Epoch 32/200, Loss: 7.684517323970795
Epoch 33/200, Loss: 7.322316646575928
Epoch 34/200, Loss: 7.302568823099136
Epoch 35/200, Loss: 7.202676296234131
Epoch 36/200, Loss: 7.017257452011108
Epoch 37/200, Loss: 6.989226698875427
Epoch 38/200, Loss: 6.75214010477066
Epoch 39/200, Loss: 6.966888725757599
Epoch 40/200, Loss: 6.774044394493103
Epoch 41/200, Loss: 6.623157560825348
Epoch 42/200, Loss: 6.966163337230682
Epoch 43/200, Loss: 6.491656601428986
Epoch 44/200, Loss: 6.58054131269455
Epoch 45/200, Loss: 6.592952162027359
Epoch 46/200, Loss: 6.6901702880859375
Epoch 47/200, Loss: 6.694109261035919
Epoch 48/200, Loss: 6.476589143276215
Epoch 49/200, Loss: 6.350058287382126
Epoch 50/200, Loss: 6.444573491811752
Epoch 51/200, Loss: 6.331862837076187
Epoch 52/200, Loss: 6.129972368478775
Epoch 53/200, Loss: 6.147720366716385
Epoch 54/200, Loss: 6.063701719045639
Epoch 55/200, Loss: 6.042039275169373
Epoch 56/200, Loss: 5.99823322892189
Epoch 57/200, Loss: 6.002959132194519
Epoch 58/200, Loss: 5.8395141661167145
Epoch 59/200, Loss: 5.818310260772705
Epoch 60/200, Loss: 5.789759129285812
Epoch 61/200, Loss: 5.710649490356445
Epoch 62/200, Loss: 5.818858981132507
Epoch 63/200, Loss: 5.8526284992694855
Epoch 64/200, Loss: 5.724664926528931
Epoch 65/200, Loss: 5.723703682422638
Epoch 66/200, Loss: 5.671028316020966
Epoch 67/200, Loss: 5.522206664085388
Epoch 68/200, Loss: 5.681692838668823
Epoch 69/200, Loss: 5.727899223566055
Epoch 70/200, Loss: 5.4400718212127686
Epoch 71/200, Loss: 5.365132629871368
Epoch 72/200, Loss: 5.378132879734039
Epoch 73/200, Loss: 5.473534196615219
Epoch 74/200, Loss: 5.551332741975784
Epoch 75/200, Loss: 5.539377868175507
Epoch 76/200, Loss: 5.227358728647232
Epoch 77/200, Loss: 5.276568979024887
Epoch 78/200, Loss: 5.20708093047142
Epoch 79/200, Loss: 5.190232038497925
Epoch 80/200, Loss: 4.99672994017601
Epoch 81/200, Loss: 4.982776165008545
Epoch 82/200, Loss: 4.965428918600082
Epoch 83/200, Loss: 5.047798663377762
Epoch 84/200, Loss: 5.003850102424622
Epoch 85/200, Loss: 4.935445487499237
Epoch 86/200, Loss: 4.98698017001152
Epoch 87/200, Loss: 4.966045379638672
Epoch 88/200, Loss: 4.985575437545776
Epoch 89/200, Loss: 4.756953716278076
Epoch 90/200, Loss: 4.817126125097275
Epoch 91/200, Loss: 4.956672430038452
C:\Users\mateo\Desktop\ABBY GOES HERE\bees\scripts\mlp.py:99: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  X = np.array([data[0] for data in dataset])
C:\Users\mateo\Desktop\ABBY GOES HERE\bees\scripts\mlp.py:99: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  X = np.array([data[0] for data in dataset])
C:\Users\mateo\Desktop\ABBY GOES HERE\bees\scripts\mlp.py:107: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  train_set = [(torch.tensor(X_train[i]), torch.tensor(y_train[i])) for i in range(len(X_train))]
C:\Users\mateo\Desktop\ABBY GOES HERE\bees\scripts\mlp.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_set = [(torch.tensor(X_val[i]), torch.tensor(y_val[i])) for i in range(len(X_val))]
Epoch 92/200, Loss: 4.914550334215164
Epoch 93/200, Loss: 4.968528717756271
Epoch 94/200, Loss: 4.941233545541763
Epoch 95/200, Loss: 4.614701181650162
Epoch 96/200, Loss: 4.699283242225647
Epoch 97/200, Loss: 4.712045431137085
Epoch 98/200, Loss: 4.7529290318489075
Epoch 99/200, Loss: 4.437609896063805
Epoch 100/200, Loss: 4.5142165422439575
Epoch 101/200, Loss: 4.7682985663414
Epoch 102/200, Loss: 4.749415427446365
Epoch 103/200, Loss: 4.640374541282654
Epoch 104/200, Loss: 4.392019018530846
Epoch 105/200, Loss: 4.540465444326401
Epoch 106/200, Loss: 4.636524647474289
Epoch 107/200, Loss: 4.33355775475502
Epoch 108/200, Loss: 4.379042774438858
Epoch 109/200, Loss: 4.267260178923607
Epoch 110/200, Loss: 4.291162312030792
Epoch 111/200, Loss: 4.2847467213869095
Epoch 112/200, Loss: 4.25601390004158
Epoch 113/200, Loss: 4.1531238704919815
Epoch 114/200, Loss: 4.268159478902817
Epoch 115/200, Loss: 4.291695922613144
Epoch 116/200, Loss: 4.390933454036713
Epoch 117/200, Loss: 4.301771551370621
Epoch 118/200, Loss: 4.1413726806640625
Epoch 119/200, Loss: 4.113646417856216
Epoch 120/200, Loss: 4.33441124856472
Epoch 121/200, Loss: 4.022621616721153
Epoch 122/200, Loss: 4.049436658620834
Epoch 123/200, Loss: 4.097402349114418
Epoch 124/200, Loss: 4.088473469018936
Epoch 125/200, Loss: 4.132810860872269
Epoch 126/200, Loss: 3.9368181824684143
Epoch 127/200, Loss: 4.242655545473099
Epoch 128/200, Loss: 3.917338192462921
Epoch 129/200, Loss: 3.8298665583133698
Epoch 130/200, Loss: 3.8361971378326416
Epoch 131/200, Loss: 4.07104466855526
Epoch 132/200, Loss: 4.313957959413528
Epoch 133/200, Loss: 3.980174019932747
Epoch 134/200, Loss: 3.9888619482517242
Epoch 135/200, Loss: 3.922623857855797
Epoch 136/200, Loss: 3.856276720762253
Epoch 137/200, Loss: 3.8420523405075073
Epoch 138/200, Loss: 3.9717709720134735
Epoch 139/200, Loss: 3.6596841663122177
Epoch 140/200, Loss: 3.727348268032074
Epoch 141/200, Loss: 3.7131952345371246
Epoch 142/200, Loss: 3.785765379667282
Epoch 143/200, Loss: 3.7799254804849625
Epoch 144/200, Loss: 3.692315623164177
Epoch 145/200, Loss: 3.6031534373760223
Epoch 146/200, Loss: 3.6157901734113693
Epoch 147/200, Loss: 3.6112151443958282
Epoch 148/200, Loss: 3.5849446952342987
Epoch 149/200, Loss: 3.9594384133815765
Epoch 150/200, Loss: 3.647504299879074
Epoch 151/200, Loss: 3.7787612080574036
Epoch 152/200, Loss: 3.4368135184049606
Epoch 153/200, Loss: 3.561963990330696
Epoch 154/200, Loss: 3.4368749111890793
Epoch 155/200, Loss: 3.436572790145874
Epoch 156/200, Loss: 3.4484962671995163
Epoch 157/200, Loss: 3.545215353369713
Epoch 158/200, Loss: 3.5372684746980667
Epoch 159/200, Loss: 3.408274367451668
Epoch 160/200, Loss: 3.4425850957632065
Epoch 161/200, Loss: 3.3447376489639282
Epoch 162/200, Loss: 3.3417750895023346
Epoch 163/200, Loss: 3.3961219638586044
Epoch 164/200, Loss: 3.401618152856827
Epoch 165/200, Loss: 3.3468227237462997
Epoch 166/200, Loss: 3.347420394420624
Epoch 167/200, Loss: 3.1917225271463394
Epoch 168/200, Loss: 3.2652437686920166
Epoch 169/200, Loss: 3.313872382044792
Epoch 170/200, Loss: 3.1549792289733887
Epoch 171/200, Loss: 3.2824156284332275
Epoch 172/200, Loss: 3.2756725549697876
Epoch 173/200, Loss: 3.145056873559952
Epoch 174/200, Loss: 3.1901838183403015
Epoch 175/200, Loss: 3.288538411259651
Epoch 176/200, Loss: 3.188231721520424
Epoch 177/200, Loss: 3.220120891928673
Epoch 178/200, Loss: 3.028484120965004
Epoch 179/200, Loss: 3.1425028145313263
Epoch 180/200, Loss: 3.117200404405594
Epoch 181/200, Loss: 3.0888384878635406
Epoch 182/200, Loss: 3.046853542327881
Epoch 183/200, Loss: 2.954317659139633
Epoch 184/200, Loss: 3.157850608229637
Epoch 185/200, Loss: 2.913169637322426
Epoch 186/200, Loss: 2.962790369987488
Epoch 187/200, Loss: 2.9098201543092728
Epoch 188/200, Loss: 2.9716892540454865
Epoch 189/200, Loss: 3.192656099796295
Epoch 190/200, Loss: 2.956020712852478
Epoch 191/200, Loss: 2.9830985367298126
Epoch 192/200, Loss: 2.8241908103227615
Epoch 193/200, Loss: 2.8837772607803345
Epoch 194/200, Loss: 2.8979815393686295
Epoch 195/200, Loss: 2.826643094420433
Epoch 196/200, Loss: 2.879887104034424
Epoch 197/200, Loss: 2.763622187077999
Epoch 198/200, Loss: 2.7690537124872208
Epoch 199/200, Loss: 2.741796553134918
Epoch 200/200, Loss: 3.217911273241043
Validation Accuracy: 0.8245614035087719