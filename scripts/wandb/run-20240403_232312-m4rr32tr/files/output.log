hive temp
hive humidity
hive pressure
weather temp
weather humidity
weather pressure
wind speed
cloud coverage
rain
Epoch 1/200, Loss: 5.530782341957092
Epoch 2/200, Loss: 5.298653960227966
Epoch 3/200, Loss: 5.097587585449219
Epoch 4/200, Loss: 4.963353157043457
Epoch 5/200, Loss: 4.802304267883301
Epoch 6/200, Loss: 4.630021572113037
Epoch 7/200, Loss: 4.476767539978027
Epoch 8/200, Loss: 4.320217192173004
Epoch 9/200, Loss: 4.277908682823181
Epoch 10/200, Loss: 4.2131489515304565
Epoch 11/200, Loss: 4.0433802008628845
Epoch 12/200, Loss: 4.019646644592285
Epoch 13/200, Loss: 3.9843886494636536
Epoch 14/200, Loss: 3.878384530544281
Epoch 15/200, Loss: 3.932117998600006
Epoch 16/200, Loss: 3.8964136838912964
Epoch 17/200, Loss: 3.8072155117988586
Epoch 18/200, Loss: 3.6580907106399536
Epoch 19/200, Loss: 3.678407371044159
Epoch 20/200, Loss: 3.694106936454773
Epoch 21/200, Loss: 3.6275473833084106
Epoch 22/200, Loss: 3.586479961872101
Epoch 23/200, Loss: 3.6009331941604614
Epoch 24/200, Loss: 3.611531674861908
Epoch 25/200, Loss: 3.577495038509369
Epoch 26/200, Loss: 3.4882558584213257
Epoch 27/200, Loss: 3.4983062744140625
Epoch 28/200, Loss: 3.436299443244934
Epoch 29/200, Loss: 3.423609673976898
Epoch 30/200, Loss: 3.471518576145172
Epoch 31/200, Loss: 3.4479508996009827
Epoch 32/200, Loss: 3.3352125883102417
Epoch 33/200, Loss: 3.202908515930176
Epoch 34/200, Loss: 3.337329685688019
Epoch 35/200, Loss: 3.2155612111091614
Epoch 36/200, Loss: 3.1925461888313293
Epoch 37/200, Loss: 3.2435207962989807
Epoch 38/200, Loss: 3.1923351883888245
Epoch 39/200, Loss: 3.203450560569763
Epoch 40/200, Loss: 3.112221896648407
Epoch 41/200, Loss: 3.1917523741722107
Epoch 42/200, Loss: 3.1344158053398132
Epoch 43/200, Loss: 3.1429293751716614
Epoch 44/200, Loss: 3.111323118209839
Epoch 45/200, Loss: 3.0503841042518616
Epoch 46/200, Loss: 2.976148247718811
Epoch 47/200, Loss: 3.077342391014099
Epoch 48/200, Loss: 3.0194321870803833
Epoch 49/200, Loss: 3.016419231891632
Epoch 50/200, Loss: 2.9345796704292297
Epoch 51/200, Loss: 2.958744525909424
Epoch 52/200, Loss: 3.0191139578819275
Epoch 53/200, Loss: 2.934367835521698
Epoch 54/200, Loss: 2.917364001274109
Epoch 55/200, Loss: 2.889785885810852
Epoch 56/200, Loss: 2.9414722323417664
Epoch 57/200, Loss: 2.8512864112854004
Epoch 58/200, Loss: 2.8442262411117554
Epoch 59/200, Loss: 2.8262621760368347
Epoch 60/200, Loss: 2.797018587589264
Epoch 61/200, Loss: 2.792162775993347
Epoch 62/200, Loss: 2.7991403937339783
Epoch 63/200, Loss: 2.806218683719635
Epoch 64/200, Loss: 2.754146635532379
Epoch 65/200, Loss: 2.8163331151008606
Epoch 66/200, Loss: 2.6899068355560303
Epoch 67/200, Loss: 2.744534969329834
Epoch 68/200, Loss: 2.6679545044898987
Epoch 69/200, Loss: 2.6727205514907837
Epoch 70/200, Loss: 2.737480044364929
Epoch 71/200, Loss: 2.66848623752594
Epoch 72/200, Loss: 2.596420705318451
Epoch 73/200, Loss: 2.6976343989372253
Epoch 74/200, Loss: 2.5572747588157654
Epoch 75/200, Loss: 2.5455291867256165
Epoch 76/200, Loss: 2.569118857383728
Epoch 77/200, Loss: 2.6041221618652344
Epoch 78/200, Loss: 2.571560561656952
Epoch 79/200, Loss: 2.5811270475387573
Epoch 80/200, Loss: 2.565800726413727
Epoch 81/200, Loss: 2.5229857563972473
Epoch 82/200, Loss: 2.4699124693870544
Epoch 83/200, Loss: 2.513100504875183
Epoch 84/200, Loss: 2.518082559108734
Epoch 85/200, Loss: 2.474459081888199
Epoch 86/200, Loss: 2.5055882930755615
Epoch 87/200, Loss: 2.366125375032425
Epoch 88/200, Loss: 2.423272967338562
Epoch 89/200, Loss: 2.3739972710609436
Epoch 90/200, Loss: 2.4656636118888855
Epoch 91/200, Loss: 2.483435273170471
Epoch 92/200, Loss: 2.4470792412757874
Epoch 93/200, Loss: 2.4771974682807922
Epoch 94/200, Loss: 2.4578187465667725
Epoch 95/200, Loss: 2.394639790058136
Epoch 96/200, Loss: 2.4011908769607544
Epoch 97/200, Loss: 2.299974203109741
Epoch 98/200, Loss: 2.348235785961151
Epoch 99/200, Loss: 2.3110005259513855
Epoch 100/200, Loss: 2.2969449162483215
Epoch 101/200, Loss: 2.2186746895313263
Epoch 102/200, Loss: 2.305874526500702
Epoch 103/200, Loss: 2.2274416387081146
Epoch 104/200, Loss: 2.262015402317047
Epoch 105/200, Loss: 2.3960881531238556
Epoch 106/200, Loss: 2.2296634316444397
Epoch 107/200, Loss: 2.266315281391144
Epoch 108/200, Loss: 2.2543618083000183
Epoch 109/200, Loss: 2.2211987376213074
Epoch 110/200, Loss: 2.1831328570842743
Epoch 111/200, Loss: 2.241377055644989
Epoch 112/200, Loss: 2.235214740037918
Epoch 113/200, Loss: 2.181411236524582
Epoch 114/200, Loss: 2.2914211452007294
Epoch 115/200, Loss: 2.1392846405506134
Epoch 116/200, Loss: 2.278914123773575
Epoch 117/200, Loss: 2.204274207353592
Epoch 118/200, Loss: 2.158961683511734
Epoch 119/200, Loss: 2.0890384018421173
Epoch 120/200, Loss: 2.075027883052826
Epoch 121/200, Loss: 2.076277732849121
Epoch 122/200, Loss: 2.0956903100013733
Epoch 123/200, Loss: 2.1368014812469482
Epoch 124/200, Loss: 2.1321504712104797
Epoch 125/200, Loss: 2.0300762951374054
Epoch 126/200, Loss: 2.165273815393448
Epoch 127/200, Loss: 2.0644799768924713
Epoch 128/200, Loss: 2.0685659646987915
Epoch 129/200, Loss: 2.075860559940338
Epoch 130/200, Loss: 2.102253794670105
Epoch 131/200, Loss: 1.9924385845661163
Epoch 132/200, Loss: 1.9956661462783813
Epoch 133/200, Loss: 2.0902381241321564
Epoch 134/200, Loss: 1.9753775894641876
Epoch 135/200, Loss: 1.9950838387012482
Epoch 136/200, Loss: 2.066323310136795
Epoch 137/200, Loss: 1.9448286890983582
Epoch 138/200, Loss: 1.8977881968021393
Epoch 139/200, Loss: 1.9709052741527557
Epoch 140/200, Loss: 1.946248471736908
Epoch 141/200, Loss: 1.9831694662570953
Epoch 142/200, Loss: 1.9408644437789917
Epoch 143/200, Loss: 1.867729812860489
Epoch 144/200, Loss: 1.975608617067337
Epoch 145/200, Loss: 1.9701168835163116
Epoch 146/200, Loss: 2.0122590363025665
Epoch 147/200, Loss: 1.8765559494495392
Epoch 148/200, Loss: 1.9569750726222992
Epoch 149/200, Loss: 1.9416362643241882
Epoch 150/200, Loss: 1.8964462876319885
Epoch 151/200, Loss: 1.9243311882019043
Epoch 152/200, Loss: 2.011650949716568
Epoch 153/200, Loss: 1.9607702791690826
Epoch 154/200, Loss: 1.936213493347168
Epoch 155/200, Loss: 1.896864116191864
Epoch 156/200, Loss: 1.8490537703037262
Epoch 157/200, Loss: 1.9158705472946167
Epoch 158/200, Loss: 1.8363816440105438
Epoch 159/200, Loss: 1.8570351898670197
Epoch 160/200, Loss: 1.8593901693820953
Epoch 161/200, Loss: 1.864613026380539
Epoch 162/200, Loss: 1.9867638945579529
Epoch 163/200, Loss: 1.8951283991336823
Epoch 164/200, Loss: 1.8161410689353943
Epoch 165/200, Loss: 1.7778306901454926
Epoch 166/200, Loss: 1.829147845506668
Epoch 167/200, Loss: 1.8513398170471191
Epoch 168/200, Loss: 1.7603926062583923
Epoch 169/200, Loss: 1.8116434812545776
Epoch 170/200, Loss: 1.8618089258670807
Epoch 171/200, Loss: 1.7710734009742737
Epoch 172/200, Loss: 1.7883518636226654
Epoch 173/200, Loss: 1.722908467054367
Epoch 174/200, Loss: 1.7655085921287537
Epoch 175/200, Loss: 1.7079470753669739
Epoch 176/200, Loss: 1.7449390292167664
Epoch 177/200, Loss: 1.7563174366950989
Epoch 178/200, Loss: 1.6998676657676697
Epoch 179/200, Loss: 1.7085078358650208
Epoch 180/200, Loss: 1.6381071209907532
Epoch 181/200, Loss: 1.8225578367710114
Epoch 182/200, Loss: 1.7386557459831238
Epoch 183/200, Loss: 1.6572048664093018
Epoch 184/200, Loss: 1.700832575559616
Epoch 185/200, Loss: 1.750598818063736
Epoch 186/200, Loss: 1.678360790014267
Epoch 187/200, Loss: 1.6421308517456055
C:\Users\mateo\Desktop\ABBY GOES HERE\bees\scripts\mlp.py:99: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  X = np.array([data[0] for data in dataset])
C:\Users\mateo\Desktop\ABBY GOES HERE\bees\scripts\mlp.py:99: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  X = np.array([data[0] for data in dataset])
C:\Users\mateo\Desktop\ABBY GOES HERE\bees\scripts\mlp.py:107: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  train_set = [(torch.tensor(X_train[i]), torch.tensor(y_train[i])) for i in range(len(X_train))]
C:\Users\mateo\Desktop\ABBY GOES HERE\bees\scripts\mlp.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_set = [(torch.tensor(X_val[i]), torch.tensor(y_val[i])) for i in range(len(X_val))]
Epoch 188/200, Loss: 1.6341376900672913
Epoch 189/200, Loss: 1.6695141792297363
Epoch 190/200, Loss: 1.7079041302204132
Epoch 191/200, Loss: 1.7455798089504242
Epoch 192/200, Loss: 1.6231251955032349
Epoch 193/200, Loss: 1.6902895271778107
Epoch 194/200, Loss: 1.6406717896461487
Epoch 195/200, Loss: 1.6271887123584747
Epoch 196/200, Loss: 1.5887708067893982
Epoch 197/200, Loss: 1.6401983201503754
Epoch 198/200, Loss: 1.6005105376243591
Epoch 199/200, Loss: 1.618533432483673
Epoch 200/200, Loss: 1.6194009184837341
Validation Accuracy: 0.8070175438596491