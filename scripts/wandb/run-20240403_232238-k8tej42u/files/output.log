hive temp
hive humidity
hive pressure
weather temp
weather humidity
weather pressure
wind speed
cloud coverage
rain
Epoch 1/200, Loss: 24.29913032054901
Epoch 2/200, Loss: 21.653834223747253
Epoch 3/200, Loss: 19.842083632946014
Epoch 4/200, Loss: 19.04675954580307
Epoch 5/200, Loss: 18.478351294994354
Epoch 6/200, Loss: 17.72087836265564
Epoch 7/200, Loss: 17.39362210035324
Epoch 8/200, Loss: 16.86482310295105
Epoch 9/200, Loss: 16.588199079036713
Epoch 10/200, Loss: 16.246166348457336
Epoch 11/200, Loss: 16.00823438167572
Epoch 12/200, Loss: 15.557095676660538
Epoch 13/200, Loss: 15.186444073915482
Epoch 14/200, Loss: 14.83679884672165
Epoch 15/200, Loss: 14.752271145582199
Epoch 16/200, Loss: 14.594718903303146
Epoch 17/200, Loss: 14.338291615247726
Epoch 18/200, Loss: 14.04515415430069
Epoch 19/200, Loss: 13.853388160467148
Epoch 20/200, Loss: 13.5825315117836
Epoch 21/200, Loss: 13.392026513814926
Epoch 22/200, Loss: 13.175532966852188
Epoch 23/200, Loss: 13.009141117334366
Epoch 24/200, Loss: 12.833122313022614
Epoch 25/200, Loss: 12.68789690732956
Epoch 26/200, Loss: 12.511091023683548
Epoch 27/200, Loss: 12.465993046760559
Epoch 28/200, Loss: 12.351002275943756
Epoch 29/200, Loss: 12.005319088697433
Epoch 30/200, Loss: 12.058639734983444
Epoch 31/200, Loss: 11.878099024295807
Epoch 32/200, Loss: 11.660981684923172
Epoch 33/200, Loss: 11.400068461894989
Epoch 34/200, Loss: 11.355615049600601
Epoch 35/200, Loss: 11.229657769203186
Epoch 36/200, Loss: 11.227652847766876
Epoch 37/200, Loss: 10.986156702041626
Epoch 38/200, Loss: 11.017318725585938
Epoch 39/200, Loss: 10.86532410979271
Epoch 40/200, Loss: 10.8226977288723
Epoch 41/200, Loss: 10.774639040231705
Epoch 42/200, Loss: 10.577379316091537
Epoch 43/200, Loss: 10.380648389458656
Epoch 44/200, Loss: 10.376412630081177
Epoch 45/200, Loss: 10.341908738017082
Epoch 46/200, Loss: 10.36037391424179
Epoch 47/200, Loss: 9.985130965709686
Epoch 48/200, Loss: 9.902361929416656
Epoch 49/200, Loss: 9.829074412584305
Epoch 50/200, Loss: 9.880236864089966
Epoch 51/200, Loss: 9.880590409040451
Epoch 52/200, Loss: 9.580543920397758
Epoch 53/200, Loss: 9.551639050245285
Epoch 54/200, Loss: 9.37911556661129
Epoch 55/200, Loss: 9.402290552854538
Epoch 56/200, Loss: 9.308021143078804
Epoch 57/200, Loss: 9.335082054138184
Epoch 58/200, Loss: 9.174167975783348
C:\Users\mateo\Desktop\ABBY GOES HERE\bees\scripts\mlp.py:99: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  X = np.array([data[0] for data in dataset])
C:\Users\mateo\Desktop\ABBY GOES HERE\bees\scripts\mlp.py:99: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  X = np.array([data[0] for data in dataset])
C:\Users\mateo\Desktop\ABBY GOES HERE\bees\scripts\mlp.py:107: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  train_set = [(torch.tensor(X_train[i]), torch.tensor(y_train[i])) for i in range(len(X_train))]
C:\Users\mateo\Desktop\ABBY GOES HERE\bees\scripts\mlp.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_set = [(torch.tensor(X_val[i]), torch.tensor(y_val[i])) for i in range(len(X_val))]
Epoch 59/200, Loss: 9.05303205549717
Epoch 60/200, Loss: 9.019689738750458
Epoch 61/200, Loss: 8.862286910414696
Epoch 62/200, Loss: 8.837909132242203
Epoch 63/200, Loss: 9.158145666122437
Epoch 64/200, Loss: 8.775698311626911
Epoch 65/200, Loss: 8.604380771517754
Epoch 66/200, Loss: 8.574193641543388
Epoch 67/200, Loss: 8.546136945486069
Epoch 68/200, Loss: 8.495708033442497
Epoch 69/200, Loss: 8.392999172210693
Epoch 70/200, Loss: 8.29075002670288
Epoch 71/200, Loss: 8.263260871171951
Epoch 72/200, Loss: 8.297304391860962
Epoch 73/200, Loss: 8.168589502573013
Epoch 74/200, Loss: 8.170174419879913
Epoch 75/200, Loss: 8.231220856308937
Epoch 76/200, Loss: 8.067519202828407
Epoch 77/200, Loss: 8.037476748228073
Epoch 78/200, Loss: 7.9927825927734375
Epoch 79/200, Loss: 7.884768262505531
Epoch 80/200, Loss: 7.749159455299377
Epoch 81/200, Loss: 7.754479065537453
Epoch 82/200, Loss: 7.667587533593178
Epoch 83/200, Loss: 7.650699511170387
Epoch 84/200, Loss: 7.638138711452484
Epoch 85/200, Loss: 7.568631559610367
Epoch 86/200, Loss: 7.545645616948605
Epoch 87/200, Loss: 7.415828689932823
Epoch 88/200, Loss: 7.344187252223492
Epoch 89/200, Loss: 7.362484991550446
Epoch 90/200, Loss: 7.375322595238686
Epoch 91/200, Loss: 7.310941472649574
Epoch 92/200, Loss: 7.178678035736084
Epoch 93/200, Loss: 7.152900278568268
Epoch 94/200, Loss: 7.214316621422768
Epoch 95/200, Loss: 7.065510123968124
Epoch 96/200, Loss: 6.9591976925730705
Epoch 97/200, Loss: 7.066659152507782
Epoch 98/200, Loss: 6.936894997954369
Epoch 99/200, Loss: 6.960723489522934
Epoch 100/200, Loss: 6.734516113996506
Epoch 101/200, Loss: 6.794752381742001
Epoch 102/200, Loss: 6.672484904527664
Epoch 103/200, Loss: 6.6844189167022705
Epoch 104/200, Loss: 6.69181402027607
Epoch 105/200, Loss: 6.5938523560762405
Epoch 106/200, Loss: 6.529406771063805
Epoch 107/200, Loss: 6.524323001503944
Epoch 108/200, Loss: 6.577639698982239
Epoch 109/200, Loss: 6.42811182141304
Epoch 110/200, Loss: 6.367685593664646
Epoch 111/200, Loss: 6.362107962369919
Epoch 112/200, Loss: 6.36388273537159
Epoch 113/200, Loss: 6.26113586127758
Epoch 114/200, Loss: 6.247767701745033
Epoch 115/200, Loss: 6.237297609448433
Epoch 116/200, Loss: 6.213761791586876
Epoch 117/200, Loss: 6.154310882091522
Epoch 118/200, Loss: 6.141519747674465
Epoch 119/200, Loss: 6.058755919337273
Epoch 120/200, Loss: 6.019854694604874
Epoch 121/200, Loss: 6.260779112577438
Epoch 122/200, Loss: 5.953403674066067
Epoch 123/200, Loss: 6.091346770524979
Epoch 124/200, Loss: 5.9738622680306435
Epoch 125/200, Loss: 5.948893800377846
Epoch 126/200, Loss: 5.826381713151932
Epoch 127/200, Loss: 5.763408839702606
Epoch 128/200, Loss: 5.713890768587589
Epoch 129/200, Loss: 5.691433295607567
Epoch 130/200, Loss: 5.70378652215004
Epoch 131/200, Loss: 5.589723750948906
Epoch 132/200, Loss: 5.537678427994251
Epoch 133/200, Loss: 5.616680085659027
Epoch 134/200, Loss: 5.570853568613529
Epoch 135/200, Loss: 5.47457030415535
Epoch 136/200, Loss: 5.465204402804375
Epoch 137/200, Loss: 5.513765811920166
Epoch 138/200, Loss: 5.432167693972588
Epoch 139/200, Loss: 5.448927730321884
Epoch 140/200, Loss: 5.36436802148819
Epoch 141/200, Loss: 5.356131628155708
Epoch 142/200, Loss: 5.256241276860237
Epoch 143/200, Loss: 5.295545369386673
Epoch 144/200, Loss: 5.365664526820183
Epoch 145/200, Loss: 5.154069267213345
Epoch 146/200, Loss: 5.207716032862663
Epoch 147/200, Loss: 5.1564271450042725
Epoch 148/200, Loss: 5.097269162535667
Epoch 149/200, Loss: 5.00721250474453
Epoch 150/200, Loss: 5.033369615674019
Epoch 151/200, Loss: 5.064690984785557
Epoch 152/200, Loss: 4.945364907383919
Epoch 153/200, Loss: 5.012137286365032
Epoch 154/200, Loss: 4.913863971829414
Epoch 155/200, Loss: 4.848636224865913
Epoch 156/200, Loss: 4.867399141192436
Epoch 157/200, Loss: 4.819064415991306
Epoch 158/200, Loss: 4.777052618563175
Epoch 159/200, Loss: 4.804431848227978
Epoch 160/200, Loss: 4.7383458986878395
Epoch 161/200, Loss: 4.720731854438782
Epoch 162/200, Loss: 4.769969053566456
Epoch 163/200, Loss: 4.705625765025616
Epoch 164/200, Loss: 4.702625945210457
Epoch 165/200, Loss: 4.72526241093874
Epoch 166/200, Loss: 4.65325091779232
Epoch 167/200, Loss: 4.62886056303978
Epoch 168/200, Loss: 4.664194941520691
Epoch 169/200, Loss: 4.572250299155712
Epoch 170/200, Loss: 4.553254418075085
Epoch 171/200, Loss: 4.450855165719986
Epoch 172/200, Loss: 4.410664416849613
Epoch 173/200, Loss: 4.430713377892971
Epoch 174/200, Loss: 4.547159597277641
Epoch 175/200, Loss: 4.40674751996994
Epoch 176/200, Loss: 4.318609498441219
Epoch 177/200, Loss: 4.361135691404343
Epoch 178/200, Loss: 4.2818456664681435
Epoch 179/200, Loss: 4.321431793272495
Epoch 180/200, Loss: 4.19244684278965
Epoch 181/200, Loss: 4.348465034738183
Epoch 182/200, Loss: 4.214782923460007
Epoch 183/200, Loss: 4.167296893894672
Epoch 184/200, Loss: 4.154462791979313
Epoch 185/200, Loss: 4.1305141150951385
Epoch 186/200, Loss: 4.090120930224657
Epoch 187/200, Loss: 4.12336240708828
Epoch 188/200, Loss: 4.169796634465456
Epoch 189/200, Loss: 4.155151642858982
Epoch 190/200, Loss: 4.098990373313427
Epoch 191/200, Loss: 4.012055486440659
Epoch 192/200, Loss: 3.93590646982193
Epoch 193/200, Loss: 4.012673441320658
Epoch 194/200, Loss: 3.90411177277565
Epoch 195/200, Loss: 3.886230058968067
Epoch 196/200, Loss: 3.8970142155885696
Epoch 197/200, Loss: 3.81450617313385
Epoch 198/200, Loss: 3.850208565592766
Epoch 199/200, Loss: 3.790631301701069
Epoch 200/200, Loss: 3.8804249316453934
Validation Accuracy: 0.7368421052631579