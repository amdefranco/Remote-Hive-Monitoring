C:\Users\mateo\Desktop\ABBY GOES HERE\bees\scripts\mlp.py:97: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  X = np.array([data[0] for data in dataset])
C:\Users\mateo\Desktop\ABBY GOES HERE\bees\scripts\mlp.py:97: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  X = np.array([data[0] for data in dataset])
C:\Users\mateo\Desktop\ABBY GOES HERE\bees\scripts\mlp.py:105: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  train_set = [(torch.tensor(X_train[i]), torch.tensor(y_train[i])) for i in range(len(X_train))]
C:\Users\mateo\Desktop\ABBY GOES HERE\bees\scripts\mlp.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_set = [(torch.tensor(X_val[i]), torch.tensor(y_val[i])) for i in range(len(X_val))]
hive temp
hive humidity
hive pressure
weather temp
weather humidity
weather pressure
wind speed
cloud coverage
rain
Epoch 1/200, Loss: 13.900802850723267
Epoch 2/200, Loss: 12.714605808258057
Epoch 3/200, Loss: 11.899951577186584
Epoch 4/200, Loss: 11.089365661144257
Epoch 5/200, Loss: 10.772951781749725
Epoch 6/200, Loss: 10.730818092823029
Epoch 7/200, Loss: 9.89456331729889
Epoch 8/200, Loss: 10.08654510974884
Epoch 9/200, Loss: 9.719683766365051
Epoch 10/200, Loss: 9.520922362804413
Epoch 11/200, Loss: 9.499533355236053
Epoch 12/200, Loss: 9.373015284538269
Epoch 13/200, Loss: 9.158284783363342
Epoch 14/200, Loss: 9.189661622047424
Epoch 15/200, Loss: 9.025802254676819
Epoch 16/200, Loss: 8.864650845527649
Epoch 17/200, Loss: 8.504005491733551
Epoch 18/200, Loss: 8.430116295814514
Epoch 19/200, Loss: 8.11869990825653
Epoch 20/200, Loss: 8.636556327342987
Epoch 21/200, Loss: 8.086112260818481
Epoch 22/200, Loss: 8.281055629253387
Epoch 23/200, Loss: 8.114988803863525
Epoch 24/200, Loss: 7.869981348514557
Epoch 25/200, Loss: 7.6698973178863525
Epoch 26/200, Loss: 7.766025245189667
Epoch 27/200, Loss: 7.664853036403656
Epoch 28/200, Loss: 7.831034779548645
Epoch 29/200, Loss: 7.37950211763382
Epoch 30/200, Loss: 7.1125448346138
Epoch 31/200, Loss: 7.407537996768951
Epoch 32/200, Loss: 7.245429217815399
Epoch 33/200, Loss: 6.860403299331665
Epoch 34/200, Loss: 7.0147488713264465
Epoch 35/200, Loss: 6.9205469489097595
Epoch 36/200, Loss: 6.8917059898376465
Epoch 37/200, Loss: 6.819510221481323
Epoch 38/200, Loss: 6.586804240942001
Epoch 39/200, Loss: 6.699280858039856
Epoch 40/200, Loss: 6.667044997215271
Epoch 41/200, Loss: 6.43944787979126
Epoch 42/200, Loss: 6.5515842735767365
Epoch 43/200, Loss: 6.622687578201294
Epoch 44/200, Loss: 6.6547587513923645
Epoch 45/200, Loss: 6.330680668354034
Epoch 46/200, Loss: 6.3170365691185
Epoch 47/200, Loss: 6.238021612167358
Epoch 48/200, Loss: 6.250182271003723
Epoch 49/200, Loss: 6.138589233160019
Epoch 50/200, Loss: 6.051780104637146
Epoch 51/200, Loss: 6.42075389623642
Epoch 52/200, Loss: 6.180237710475922
Epoch 53/200, Loss: 6.248473137617111
Epoch 54/200, Loss: 6.119649648666382
Epoch 55/200, Loss: 5.832137942314148
Epoch 56/200, Loss: 5.891312807798386
Epoch 57/200, Loss: 5.873439788818359
Epoch 58/200, Loss: 5.870754420757294
Epoch 59/200, Loss: 6.027258455753326
Epoch 60/200, Loss: 5.639068692922592
Epoch 61/200, Loss: 5.765228420495987
Epoch 62/200, Loss: 5.55160117149353
Epoch 63/200, Loss: 5.81351175904274
Epoch 64/200, Loss: 5.462583661079407
Epoch 65/200, Loss: 5.478871315717697
Epoch 66/200, Loss: 5.727028012275696
Epoch 67/200, Loss: 5.660693049430847
Epoch 68/200, Loss: 5.621639847755432
Epoch 69/200, Loss: 5.318313717842102
Epoch 70/200, Loss: 5.803030878305435
Epoch 71/200, Loss: 5.352817893028259
Epoch 72/200, Loss: 5.318240195512772
Epoch 73/200, Loss: 5.31564262509346
Epoch 74/200, Loss: 5.225474715232849
Epoch 75/200, Loss: 5.367341548204422
Epoch 76/200, Loss: 5.1506966054439545
Epoch 77/200, Loss: 5.046342372894287
Epoch 78/200, Loss: 5.044549882411957
Epoch 79/200, Loss: 5.116366922855377
Epoch 80/200, Loss: 4.896612733602524
Epoch 81/200, Loss: 5.3624933660030365
Epoch 82/200, Loss: 5.012484282255173
Epoch 83/200, Loss: 5.161450535058975
Epoch 84/200, Loss: 5.24529755115509
Epoch 85/200, Loss: 5.102961212396622
Epoch 86/200, Loss: 5.051724314689636
Epoch 87/200, Loss: 4.956266075372696
Epoch 88/200, Loss: 4.733730271458626
Epoch 89/200, Loss: 4.9856970608234406
Epoch 90/200, Loss: 4.983229160308838
Epoch 91/200, Loss: 4.735787808895111
Epoch 92/200, Loss: 5.278960675001144
Epoch 93/200, Loss: 4.710871875286102
Epoch 94/200, Loss: 4.7083233296871185
Epoch 95/200, Loss: 4.645702034235001
Epoch 96/200, Loss: 4.552351161837578
Epoch 97/200, Loss: 4.669792681932449
Epoch 98/200, Loss: 4.559185177087784
Epoch 99/200, Loss: 4.801290988922119
Epoch 100/200, Loss: 4.883326977491379
Epoch 101/200, Loss: 4.453629404306412
Epoch 102/200, Loss: 4.902974456548691
Epoch 103/200, Loss: 4.624329775571823
Epoch 104/200, Loss: 4.581121563911438
Epoch 105/200, Loss: 4.500803589820862
Epoch 106/200, Loss: 4.5337154269218445
Epoch 107/200, Loss: 4.533464401960373
Epoch 108/200, Loss: 4.271728843450546
Epoch 109/200, Loss: 4.508802890777588
Epoch 110/200, Loss: 4.571627587080002
Epoch 111/200, Loss: 4.38315162062645
Epoch 112/200, Loss: 4.562617987394333
Epoch 113/200, Loss: 4.48748017847538
Epoch 114/200, Loss: 4.296520292758942
Epoch 115/200, Loss: 4.356311917304993
Epoch 116/200, Loss: 4.156592845916748
Epoch 117/200, Loss: 4.4326281398534775
Epoch 118/200, Loss: 4.0852030813694
Epoch 119/200, Loss: 4.398351043462753
Epoch 120/200, Loss: 4.257296904921532
Epoch 121/200, Loss: 4.066483125090599
Epoch 122/200, Loss: 4.129455000162125
Epoch 123/200, Loss: 4.2162520587444305
Epoch 124/200, Loss: 4.153165012598038
Epoch 125/200, Loss: 4.158956155180931
Epoch 126/200, Loss: 4.3196806609630585
Epoch 127/200, Loss: 4.188088983297348
Epoch 128/200, Loss: 4.187037989497185
Epoch 129/200, Loss: 4.068047732114792
Epoch 130/200, Loss: 4.111573323607445
Epoch 131/200, Loss: 4.026055753231049
Epoch 132/200, Loss: 3.966369077563286
Epoch 133/200, Loss: 4.020308494567871
Epoch 134/200, Loss: 3.943434312939644
Epoch 135/200, Loss: 4.034307658672333
Epoch 136/200, Loss: 3.831643834710121
Epoch 137/200, Loss: 4.024319335818291
Epoch 138/200, Loss: 3.825671136379242
Epoch 139/200, Loss: 3.961336851119995
Epoch 140/200, Loss: 3.9668561220169067
Epoch 141/200, Loss: 4.158328711986542
Epoch 142/200, Loss: 3.8379377126693726
Epoch 143/200, Loss: 3.9003928005695343
Epoch 144/200, Loss: 3.768893286585808
Epoch 145/200, Loss: 3.633058100938797
Epoch 146/200, Loss: 3.689850702881813
Epoch 147/200, Loss: 3.732848361134529
Epoch 148/200, Loss: 3.784460961818695
Epoch 149/200, Loss: 3.5997989624738693
Epoch 150/200, Loss: 3.6819216459989548
Epoch 151/200, Loss: 3.690818503499031
Epoch 152/200, Loss: 3.9121236503124237
Epoch 153/200, Loss: 3.463925488293171
Epoch 154/200, Loss: 3.690313398838043
Epoch 155/200, Loss: 3.672313168644905
Epoch 156/200, Loss: 3.853942960500717
Epoch 157/200, Loss: 3.4507732167840004
Epoch 158/200, Loss: 3.5736301988363266
Epoch 159/200, Loss: 3.4943151473999023
Epoch 160/200, Loss: 3.5593309700489044
Epoch 161/200, Loss: 3.4239865839481354
Epoch 162/200, Loss: 3.5250270068645477
Epoch 163/200, Loss: 3.515592575073242
Epoch 164/200, Loss: 3.4234434962272644
Epoch 165/200, Loss: 3.404188483953476
Epoch 166/200, Loss: 3.7064071744680405
Epoch 167/200, Loss: 3.748351663351059
Epoch 168/200, Loss: 3.4562822729349136
Epoch 169/200, Loss: 3.703316390514374
Epoch 170/200, Loss: 3.4890601187944412
Epoch 171/200, Loss: 3.2888780534267426
Epoch 172/200, Loss: 3.3805138170719147
Epoch 173/200, Loss: 3.359430819749832
Epoch 174/200, Loss: 3.4173571169376373
Epoch 175/200, Loss: 3.4203931987285614
Epoch 176/200, Loss: 3.269603595137596
Epoch 177/200, Loss: 3.211048275232315
Epoch 178/200, Loss: 3.2213039696216583
Epoch 179/200, Loss: 3.230022609233856
Epoch 180/200, Loss: 3.4063413441181183
Epoch 181/200, Loss: 3.267139256000519
Epoch 182/200, Loss: 3.2203172594308853
Epoch 183/200, Loss: 3.247894734144211
Epoch 184/200, Loss: 3.3133260160684586
Epoch 185/200, Loss: 3.3335073739290237
Epoch 186/200, Loss: 3.2893384844064713
Epoch 187/200, Loss: 3.289439246058464
Epoch 188/200, Loss: 3.047416791319847
Epoch 189/200, Loss: 3.238366112112999
Epoch 190/200, Loss: 3.0719888657331467
Epoch 191/200, Loss: 3.3074821382761
Epoch 192/200, Loss: 3.0863019675016403
Epoch 193/200, Loss: 3.0431623458862305
Epoch 194/200, Loss: 3.180338904261589
Epoch 195/200, Loss: 3.1414175033569336
Epoch 196/200, Loss: 3.574971094727516
Epoch 197/200, Loss: 3.3299894630908966
Epoch 198/200, Loss: 3.0699407309293747
Epoch 199/200, Loss: 3.3229391872882843
Epoch 200/200, Loss: 3.0131225883960724
Validation Accuracy: 0.7719298245614035